# -*- coding: utf-8 -*-
"""gpt-4o-mini vs Gemini 1.5 Pro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nDNJHE5Sg4IEa8iVs2-pswkeaLiAaJEn
"""

!pip install openai google-generativeai pandas matplotlib

import time
import openai
import google.generativeai as genai
import pandas as pd
import matplotlib.pyplot as plt

os.environ["OPENAI_API_KEY"] = "yourAPIkey"
openai.api_key = os.getenv("OPENAI_API_KEY")
genai.configure(api_key="yourAPIkey")

def  query_gpt4o(question):
    """Query ChatGPT API and measure response time."""
    start_time = time.time()
    client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))  # Initialize OpenAI client

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": question}],
        max_tokens=100
    )

    response_time = time.time() - start_time
    answer = response.choices[0].message.content.strip()  # Fixed response parsing
    return answer, response_time

def query_gemini(question):
    """Query Gemini 1.5 Pro and measure response time."""
    time.sleep(2)
    start_time = time.time()
    model = genai.GenerativeModel("gemini-1.5-pro")
    response = model.generate_content(question)
    response_time = time.time() - start_time
    answer = response.text.strip() if response.text else "Error fetching response"
    return answer, response_time

!pip install datasets

from datasets import load_dataset

# Load the math_qa dataset
dataset = load_dataset("math_qa", split="train")

# Extract the first 100 questions
questions = dataset["Problem"][:100]

# Print first 5 questions for preview
print(questions[:5])

data = []

for i, question in enumerate(questions):
    print(f"Processing question {i+1}/{len(questions)}...")
    gpt_answer, gpt_time = query_gpt4o(question)
    gemini_answer, gemini_time = query_gemini(question)
    data.append([question, gpt_answer, gpt_time, gemini_answer, gemini_time])

# Convert to DataFrame
df = pd.DataFrame(data, columns=["Question", "GPT-4o Answer", "GPT-4o Time (s)", "Gemini Answer", "Gemini Time (s)"])

# Save to CSV
df.to_csv("gpt4o_vs_gemini_flash_results.csv", index=False)

print(df)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the CSV
df = pd.read_csv("gpt4o_vs_gemini_flash_results.csv")

# Convert time columns to float
df["GPT-4o Time (s)"] = df["GPT-4o Time (s)"].astype(float)
df["Gemini Time (s)"] = df["Gemini Time (s)"].astype(float)

# Set Seaborn style
sns.set_theme(style="whitegrid", palette="pastel")

# Create a figure with subplots
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle("GPT-4o-mini vs Gemini 1.5 Pro - Performance Comparison", fontsize=16, fontweight="bold")

# 1. Response Time Line Plot
sns.lineplot(x=df.index, y=df["GPT-4o Time (s)"], marker='o', label="GPT-4o-mini", ax=axes[0, 0])
sns.lineplot(x=df.index, y=df["Gemini Time (s)"], marker='s', label="Gemini 1.5 Pro", ax=axes[0, 0])
axes[0, 0].set_title("Response Time Comparison")
axes[0, 0].set_xlabel("Question Number")
axes[0, 0].set_ylabel("Response Time (s)")

# 2. Boxplot for Response Time Distribution
sns.boxplot(data=df[["GPT-4o Time (s)", "Gemini Time (s)"]], ax=axes[0, 1])
axes[0, 1].set_title("Response Time Distribution")
axes[0, 1].set_ylabel("Time (s)")

# 3. Kernel Density Estimation (KDE) Plot
sns.kdeplot(df["GPT-4o Time (s)"], fill=True, label="GPT-4o-mini", ax=axes[1, 0])
sns.kdeplot(df["Gemini Time (s)"], fill=True, label="Gemini 1.5 Pro", ax=axes[1, 0])
axes[1, 0].set_title("Response Time Density Plot")
axes[1, 0].set_xlabel("Response Time (s)")
axes[1, 0].legend()

# 4. Bar Chart for Average Response Time
avg_times = df[["GPT-4o Time (s)", "Gemini Time (s)"]].mean()
sns.barplot(x=avg_times.index, y=avg_times.values, ax=axes[1, 1], palette="coolwarm")
axes[1, 1].set_title("Average Response Time Comparison")
axes[1, 1].set_ylabel("Average Time (s)")

plt.tight_layout(rect=[0, 0.03, 1, 0.97])
plt.show()