ðŸ“Š GPT-4o Mini vs. Gemini 1.5 Pro â€“ Performance Benchmark
ðŸš€ Overview
This repository contains a comparative analysis of GPT-4o Mini and Gemini 1.5 Pro using 100 math-related questions from the MathQA dataset. The goal is to evaluate their response times and visualize the results.

ðŸ“Œ Key Highlights
Dataset Used: MathQA (100 questions)
Models Compared: GPT-4o Mini & Gemini 1.5 Pro
Metric Analyzed: Response time (in seconds)
Visualizations: Response time trends, distributions, and density plots

ðŸ“Š Visualizations
The analysis includes various visualizations, such as:
âœ… Response Time Trends â€“ How response times vary per question
âœ… Distribution Analysis â€“ Box plots for overall response time comparison
âœ… Density Plots â€“ Probability distribution of response times
âœ… Average Response Time Comparison â€“ Which model is generally faster

ðŸ“ˆ Results & Insights
Gemini 1.5 Pro showed lower average response times than GPT-4o Mini.
GPT-4o Mini exhibited higher variance, meaning its speed was inconsistent across different questions.
Further analysis is required to evaluate accuracy and reasoning ability alongside speed.
ðŸ“¢ Contributions & Discussions
If you find this work interesting or have ideas for improvement, feel free to open an issue or contribute! Letâ€™s make AI benchmarking more insightful together. ðŸš€

ðŸ“Œ Letâ€™s connect on LinkedIn & GitHub for more AI experiments!
